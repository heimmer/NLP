{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzLErpvVWIjnMbtFsbV8NY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heimmer/NLP/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quention 1  "
      ],
      "metadata": {
        "id": "KWv9B3S3KISK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1\n",
        "**answer**  \n",
        "- **vanishing/exploding gradient**:   \n",
        "Loss function of RNN is defined as sum of the loss at every time step. After decomposing the loss at each time step according to the chain rule, $\\frac{∂h_t}{∂h1}$ is the only term which is hard to compute, we need to further decompose it to $\\frac{∂h_t}{∂h_{t-1}}\\frac{∂h_{t-1}}{∂h_{t-2}}...\\frac{∂h2}{∂h1}$. It's fine that each $\\frac{∂h_t}{∂h_{t-1}}$ is a bit larger or smaller than 1, but in RNN, the time step T can be large. After a expotential calculation of power T, the gradient can be very large or small, which is the so-called vanishing/exploding gradient problem.  \n",
        "- **long-term dependency**  \n",
        "Human language system is complicated, sometimes information related to the t-th word appeared very early in the sentence. If the gradient is small, the model can't learn this dependency, which is so-called long-term dependency problem."
      ],
      "metadata": {
        "id": "sz1opys1KOXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2  \n",
        "**answer**  \n",
        "Yes, LSTM can solve the problems of vanishing and exploding gradient.  \n",
        "The recursive derivative in Vanilla RNN is the main reason of vanishing/exploding gradient.But LSTM introduces a set of gating units and a cell state to control the memory intead of adding up all history.  \n",
        "- **forget gate** controls what is kept vs forgotten from previous cell state  \n",
        "- **input gate** controls what part of new cell content are written to the cell\n",
        "- **output gate** controls what parts of cell are output to hidden state  \n",
        "\n",
        "Those gates can help to adjust and prevent the gradients from vanishing/exploding"
      ],
      "metadata": {
        "id": "geAO_v1pXG1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3  \n",
        "**answer** \n",
        "$4*[(d_i+d_o)*d_o+d_o]$  \n",
        "- in each LSTM cell, there are 4 non-linear transformations (3 gates and 1 tanh)\n",
        "- dim of input is $d_i$, dim of hidden state from previous LSTM cell is $d_o$\n",
        "- at first, we need to combine them together, so dim of the combination is $d_i+d_o$  \n",
        "- output of each non-linear transformation $σ$ is of dim $d_o$, and dim of bias is also $d_o$\n",
        "- so number of parameters of each non-linear transformation is $(d_i+d_o)*d_o+d_o$\n",
        "- 4 non-linear transformations are different, they don't share parameters, so number of parameters of the whole cell is $4*[(d_i+d_o)*d_o+d_o]$"
      ],
      "metadata": {
        "id": "-si8IvKbmfSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4  \n",
        "**answer** : 12585  \n",
        "- vocab_size=1000, embedding_dim=10, so number of parameters in embedding matrix is 1000*10=10000\n",
        "- embedding_dim=10, so input_size of LSTM cell is 10; hidden_size=20, according to last question, number of parameters in LSTM cell is 4*[(10+20)*20+20]=2480\n",
        "- hidden_size=20, num_class=5, so number of parameters in decoder is 20*5+5=105\n",
        "- total number of parameters is 10000+2480+105 = 12585"
      ],
      "metadata": {
        "id": "0fF1_BBWxibf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA7D8hcyIs8W"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self, vocab_size=1000, embedding_dim=10,\n",
        "        hidden_size=20, num_class=5):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size,\n",
        "    bidirectional=True, num_layers=1)\n",
        "    self.decoder = nn.Linear(hidden_size, num_class, bias=True)\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters()"
      ],
      "metadata": {
        "id": "hHR1mwi2xnOK",
        "outputId": "37f4168a-2271-4b1c-e9d9-3a1f700b6c37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f7d03300ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.named_parameters():\n",
        "    print(param[0])"
      ],
      "metadata": {
        "id": "vgxjg0tQzY8M",
        "outputId": "4067d43f-04dd-442a-d801-698c06eac668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings.weight\n",
            "lstm.weight_ih_l0\n",
            "lstm.weight_hh_l0\n",
            "lstm.bias_ih_l0\n",
            "lstm.bias_hh_l0\n",
            "lstm.weight_ih_l0_reverse\n",
            "lstm.weight_hh_l0_reverse\n",
            "lstm.bias_ih_l0_reverse\n",
            "lstm.bias_hh_l0_reverse\n",
            "decoder.weight\n",
            "decoder.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param.shape)"
      ],
      "metadata": {
        "id": "mHYZG1XkxpAA",
        "outputId": "8cfae095-c287-453c-d6c1-d57a6fbf3d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 10])\n",
            "torch.Size([80, 10])\n",
            "torch.Size([80, 20])\n",
            "torch.Size([80])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 10])\n",
            "torch.Size([80, 20])\n",
            "torch.Size([80])\n",
            "torch.Size([80])\n",
            "torch.Size([5, 20])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quention 2"
      ],
      "metadata": {
        "id": "Lkxm7wH-Gv_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1  \n",
        "**answer**  \n",
        "advantages:\n",
        "- solve the long-term dependency problem, after attention layer, each output will carry information from all inputs\n",
        "- the calculation in attention layer goes parallelly as matrix multiplication, rather than sequentially, which is more computationally efficient  \n",
        "\n",
        "disadvantages:  \n",
        "- the sequential process of RNN naturally save position information, but attention layer won't save position information, it needs positional encoding to assist\n",
        "- assume length of the sequence is $L$, then complexity of attention is $L^2$. If $L$ is too large, attention will become computationally inefficient"
      ],
      "metadata": {
        "id": "DeIJdy5WI6Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2  \n",
        "**answer**  \n",
        "For large values of $d_k$, the dot products grow large in magnitude, pushing the softmax into the regions where is has extremely small gradients. So it is necessary to introduce a scaler before normalization.  \n",
        "\n",
        "Mathematically, \n",
        "- queries and keys are of dimension $d_k$, so $A=QK^T$ is a $d_k*d_k$ matrix\n",
        "- its first element is $A_{1,1}=\\sum\\limits_{i=0}^{d_k}Q_{1,i}K_{i,1}$. Q and K are random variables, so each entry of them has 0 mean and 1 variance\n",
        "- $E[Q_{1,i}K_{i,1}]=E[Q_{1,i}]E[K_{i,1}]=0$  \n",
        "  $Var(Q_{1,i}K_{i,1})=(Var(Q_{1,i}+E[Q_{1,i}]^2))(Var(K_{i,1}+E[K_{i,1}]^2))-E[Q_{1,i}]^2E[K_{i,1}]^2=1$\n",
        "- $E[A_{1,1}]=\\sum\\limits_{i=0}^{d_k}E[Q_{1,i}K_{i,1}]=0$\n",
        "- $Var(A_{1,1})=\\sum\\limits_{i=0}^{d_k}Var(Q_{1,i}K_{i,1})=d_k$\n",
        "- to scale each entry of A to a reasonable value which is suitable for softmax, we devide each entry with its std, which is equal to multiple with $\\frac{1}{\\sqrt{d_k}}$\n",
        "\n"
      ],
      "metadata": {
        "id": "bKNhj81rI589"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aI3rI53aI52i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n7wnZZxtI5zf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pB06OU0LI5jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8mKFMJtI5al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAF5tnHPGzHL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}