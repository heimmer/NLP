{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2TL3oA36HWicGAyKQPecR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heimmer/NLP/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quention 1  "
      ],
      "metadata": {
        "id": "KWv9B3S3KISK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1\n",
        "**answer**  \n",
        "- **vanishing/exploding gradient**:   \n",
        "Loss function of RNN is defined as sum of the loss at every time step. After decomposing the loss at each time step according to the chain rule, $\\frac{∂h_t}{∂h1}$ is the only term which is hard to compute, we need to further decompose it to $\\frac{∂h_t}{∂h_{t-1}}\\frac{∂h_{t-1}}{∂h_{t-2}}...\\frac{∂h2}{∂h1}$. It's fine that each $\\frac{∂h_t}{∂h_{t-1}}$ is a bit larger or smaller than 1, but in RNN, the time step T can be large. After a expotential calculation of power T, the gradient can be very large or small, which is the so-called vanishing/exploding gradient problem.  \n",
        "- **long-term dependency**  \n",
        "Human language system is complicated, sometimes information related to the t-th word appeared very early in the sentence. If the gradient is small, the model can't learn this dependency, which is so-called long-term dependency problem."
      ],
      "metadata": {
        "id": "sz1opys1KOXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2  \n",
        "**answer**  \n",
        "Yes, LSTM can solve the problems of vanishing and exploding gradient.  \n",
        "The recursive derivative in Vanilla RNN is the main reason of vanishing/exploding gradient.But LSTM introduces a set of gating units and a cell state to control the memory intead of adding up all history.  \n",
        "- **forget gate** controls what is kept vs forgotten from previous cell state  \n",
        "- **input gate** controls what part of new cell content are written to the cell\n",
        "- **output gate** controls what parts of cell are output to hidden state  \n",
        "\n",
        "Those gates can help to adjust and prevent the gradients from vanishing/exploding"
      ],
      "metadata": {
        "id": "geAO_v1pXG1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3  \n",
        "**answer** \n",
        "$4*[(d_i+d_o)*d_o+d_o]$  \n",
        "- in each LSTM cell, there are 4 non-linear transformations (3 gates and 1 tanh)\n",
        "- dim of input is $d_i$, dim of hidden state from previous LSTM cell is $d_o$\n",
        "- at first, we need to combine them together, so dim of the combination is $d_i+d_o$  \n",
        "- output of each non-linear transformation $σ$ is of dim $d_o$, and dim of bias is also $d_o$\n",
        "- so number of parameters of each non-linear transformation is $(d_i+d_o)*d_o+d_o$\n",
        "- 4 non-linear transformations are different, they don't share parameters, so number of parameters of the whole cell is $4*[(d_i+d_o)*d_o+d_o]$"
      ],
      "metadata": {
        "id": "-si8IvKbmfSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4  \n",
        "**answer** : 12585  \n",
        "- vocab_size=1000, embedding_dim=10, so number of parameters in embedding matrix is 1000*10=10000\n",
        "- embedding_dim=10, so input_size of LSTM cell is 10; hidden_size=20, according to last question, number of parameters in LSTM cell is 4*[(10+20)*20+20]=2480\n",
        "- hidden_size=20, num_class=5, so number of parameters in decoder is 20*5+5=105\n",
        "- total number of parameters is 10000+2480+105 = 12585"
      ],
      "metadata": {
        "id": "0fF1_BBWxibf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA7D8hcyIs8W"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self, vocab_size=1000, embedding_dim=10,\n",
        "        hidden_size=20, num_class=5):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size,\n",
        "    bidirectional=True, num_layers=1)\n",
        "    self.decoder = nn.Linear(hidden_size, num_class, bias=True)\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters()"
      ],
      "metadata": {
        "id": "hHR1mwi2xnOK",
        "outputId": "37f4168a-2271-4b1c-e9d9-3a1f700b6c37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f7d03300ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.named_parameters():\n",
        "    print(param[0])"
      ],
      "metadata": {
        "id": "vgxjg0tQzY8M",
        "outputId": "4067d43f-04dd-442a-d801-698c06eac668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings.weight\n",
            "lstm.weight_ih_l0\n",
            "lstm.weight_hh_l0\n",
            "lstm.bias_ih_l0\n",
            "lstm.bias_hh_l0\n",
            "lstm.weight_ih_l0_reverse\n",
            "lstm.weight_hh_l0_reverse\n",
            "lstm.bias_ih_l0_reverse\n",
            "lstm.bias_hh_l0_reverse\n",
            "decoder.weight\n",
            "decoder.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param.shape)"
      ],
      "metadata": {
        "id": "mHYZG1XkxpAA",
        "outputId": "8cfae095-c287-453c-d6c1-d57a6fbf3d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 10])\n",
            "torch.Size([80, 10])\n",
            "torch.Size([80, 20])\n",
            "torch.Size([80])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 10])\n",
            "torch.Size([80, 20])\n",
            "torch.Size([80])\n",
            "torch.Size([80])\n",
            "torch.Size([5, 20])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quention 2"
      ],
      "metadata": {
        "id": "Lkxm7wH-Gv_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "X-1DByDXrhBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer**  \n",
        "advantages:\n",
        "- solve the long-term dependency problem, after attention layer, each output will carry information from all inputs\n",
        "- the calculation in attention layer goes parallelly as matrix multiplication, rather than sequentially, which is more computationally efficient  \n",
        "\n",
        "disadvantages:  \n",
        "- the sequential process of RNN naturally save position information, but attention layer won't save position information, it needs positional encoding to assist\n",
        "- assume length of the sequence is $L$, then complexity of attention is $L^2$. If $L$ is too large, attention will become computationally inefficient"
      ],
      "metadata": {
        "id": "DeIJdy5WI6Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2"
      ],
      "metadata": {
        "id": "ePDB6b5jrj4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer**  \n",
        "For large values of $d_k$, the dot products grow large in magnitude, pushing the softmax into the regions where is has extremely small gradients. So it is necessary to introduce a scaler before normalization.  \n",
        "\n",
        "Mathematically, \n",
        "- queries and keys are of dimension $d_k$, so $A=QK^T$ is a $d_k*d_k$ matrix\n",
        "- its first element is $A_{1,1}=\\sum\\limits_{i=0}^{d_k}Q_{1,i}K_{i,1}$. Q and K are random variables, so each entry of them has 0 mean and 1 variance\n",
        "- $E[Q_{1,i}K_{i,1}]=E[Q_{1,i}]E[K_{i,1}]=0$  \n",
        "  $Var(Q_{1,i}K_{i,1})=(Var(Q_{1,i}+E[Q_{1,i}]^2))(Var(K_{i,1}+E[K_{i,1}]^2))-E[Q_{1,i}]^2E[K_{i,1}]^2=1$\n",
        "- $E[A_{1,1}]=\\sum\\limits_{i=0}^{d_k}E[Q_{1,i}K_{i,1}]=0$\n",
        "  $Var(A_{1,1})=\\sum\\limits_{i=0}^{d_k}Var(Q_{1,i}K_{i,1})=d_k$\n",
        "- to scale each entry of A to a reasonable value which is suitable for softmax, we devide each entry with its std, which is equal to multiple with $\\frac{1}{\\sqrt{d_k}}$\n",
        "\n"
      ],
      "metadata": {
        "id": "bKNhj81rI589"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3"
      ],
      "metadata": {
        "id": "aI3rI53aI52i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer**  \n",
        "The decoder (right part) is designed to solve the auto-regressive problems.  \n",
        "More specificly, the masked self-attention part, will mask out future positions, preventing leftward information flow to preserve the auto-regressive property. Each position attends to all positions in the decoder up to and including the current position."
      ],
      "metadata": {
        "id": "n7wnZZxtI5zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "dOv1n-4vtZTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "tL-qsJX3tZQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer**  \n",
        "- **encoder-based:** are a good choice for NLU tasks, because they are able to encode input text into a fixed-size representation which contains better contextualized information, so it can be used as input to downstream tasks and produce better performance.\n",
        "- **decoder-based:** are a good choice for NLG tasks, beacause they can generate output one token at a time, which naturally fits need of NLG tasks.\n",
        "- **encoder-decoder:** are a good choice for tasks which need combination of NLU and NLG, such as machine translation. Because the input text need better understanding which encoder can satisfy, output need to be generated sequentially which decoder can satisfy."
      ],
      "metadata": {
        "id": "ASlVNrJRtZOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2"
      ],
      "metadata": {
        "id": "ABKbTacUtZK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unidirectional LM (left-to-right)**  \n",
        "\n",
        "|  | Birt | [MASK] | a0tivities | ; | He | was | [Mask] | to | jail |\n",
        "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
        "| Birt | 0 | 1 | 1 | ; | 1 | 1 | 1 | 1 | 1 |\n",
        "| [MASK] | 0 | 0 | 1 | ; | 1 | 1 | 1 | 1 | 1 |\n",
        "| a0tivities | 0 | 0 | 0 | ; | 1 | 1 | 1 | 1 | 1 |\n",
        "| -------------- | ---- | ----------- | --------------- | ---- | ---- | ------- | ----------- | ---- | ---- |\n",
        "| He | 1 | 1 | 1 | ; | 0 | 1 | 1 | 1 | 1 |\n",
        "| was | 1 | 1 | 1 | ; | 0 | 0 | 1 | 1 | 1 |\n",
        "| [Mask] | 1 | 1 | 1 | ; | 0 | 0 | 0 | 1 | 1 |\n",
        "| to | 1 | 1 | 1 | ; | 0 | 0 | 0 | 0 | 1 |\n",
        "| jail | 1 | 1 | 1 | ; | 0 | 0 | 0 | 0 | 0 |\n"
      ],
      "metadata": {
        "id": "Ba7ssB2OtZC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bidirectional LM**  \n",
        "\n",
        "|  | Birt | [MASK] | activities | ; | He | was | [Mask] | to | jail |\n",
        "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
        "| Birt | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |\n",
        "| [MASK] | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |\n",
        "| activities | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |\n",
        "| -------------- | ---- | ----------- | --------------- | ---- | ---- | ------- | ----------- | ---- | ---- |\n",
        "| He | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |\n",
        "| was | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |\n",
        "| [Mask] | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |\n",
        "| to | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |\n",
        "| jail | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |"
      ],
      "metadata": {
        "id": "ZulAJuuyyFAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequence-to-sequence LM**  \n",
        "\n",
        "|  | Birt | [MASK] | activities | ; | He | was | [Mask] | to | jail |\n",
        "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
        "| Birt | 0 | 0 | 0 | ; | 1 | 1 | 1 | 1 | 1 |\n",
        "| [MASK] | 0 | 0 | 0 | ; | 1 | 1 | 1 | 1 | 1 |\n",
        "| activities | 0 | 0 | 0 | ; | 1 | 1 | 1 | 1 | 1 |\n",
        "| -------------- | ---- | ----------- | --------------- | ---- | ---- | ------- | ----------- | ---- | ---- |\n",
        "| He | 0 | 0 | 0 | ; | 0 | 1 | 1 | 1 | 1 |\n",
        "| was | 0 | 0 | 0 | ; | 0 | 0 | 1 | 1 | 1 |\n",
        "| [Mask] | 0 | 0 | 0 | ; | 0 | 0 | 0 | 1 | 1 |\n",
        "| to | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 1 |\n",
        "| jail | 0 | 0 | 0 | ; | 0 | 0 | 0 | 0 | 0 |"
      ],
      "metadata": {
        "id": "hHHCosGRyE9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3"
      ],
      "metadata": {
        "id": "Mo0xjKr1yE6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HeecKR1LyE3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_for_unidirectional_lm(source seg, target seg):\n",
        "  split \n",
        "\n",
        "mask for bidirectional lm(source seg, target seg), mask for seq2seq lm(source seg, target seg)"
      ],
      "metadata": {
        "id": "z8mKFMJtI5al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAF5tnHPGzHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quention 4"
      ],
      "metadata": {
        "id": "llozqp6H1Wnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "jyhCWq7W1WkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**asnwer**  \n",
        "We can fine-tune BERT-based approach to SQuAD v2.0 to solve the problem that some questions have no answer. Originally in BERT, we need to generate probablities of start, end of each token ($Pstart_i,Pend_i$). To address the mentioned problem, we need to allow there is no answer, eg. when index of start is lager than the end, we say there is no reasonable answer.\n",
        "The workfolow is as follow:\n",
        "- data pre-process: tokenize and convert the dataset to a format that can be feed into BERT\n",
        "- split dataset: seperate dataset to training set, validation set, test set\n",
        "- sepecial design: \n",
        "- fine-tune: train the model on training set, evaluate on the validation set, optimize model's parameters\n",
        "- evaluate: use test set to evaluate the performance, we can use F1 and EM(exact match) as metrics"
      ],
      "metadata": {
        "id": "6uXVGhZ41WfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2"
      ],
      "metadata": {
        "id": "S-A59ouP1WUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**asnwer**  \n",
        "- **QA tasks:**   \n",
        "  objective of QA tasks is to predict the start and end position of the answer span. For BERT, we can adjust masked language modeling (MLM) to adapt QA tasks. Specificly, is to mask out answer span in input to let BERT learn the start and end position of answer span.\n",
        "- **Span-based tasks (coreference resolution):**   \n",
        "  objective of coreference resolution tasks is to identify the mention and corefer in the input. For BERT, we can adjust next sentence prediction (NSP) to adapt coreference resolution. Specificly, is to put the mention and corefer as input to let BERT learn if they refer to the same entity or not."
      ],
      "metadata": {
        "id": "QvDzFnPY60jg"
      }
    }
  ]
}