{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6493 - Tutorial 1\n",
    "## Introduction to JupyterHub and PyTorch\n",
    "\n",
    "Welcome to CS6493 tutorial. In this tutorial, you will get familiar with our exprimental environment, and also practice with some basic PyTorch operations.\n",
    "\n",
    "## 1. JupyterHub\n",
    "\n",
    "You can use the JupyterHub to run the toy models. Here are some notes for JupyterHub:\n",
    "\n",
    "- You are supposed to be familiar with Python and Jupyter;\n",
    "- Enter https://mljh.cs.cityu.edu.hk/ and login with your **EID** to use JupyterHub;\n",
    "- Each student will be allocated with **8GB GPU memories**;\n",
    "- Accommodate up to **60 students** to simultaneously use the JpuyterHub;\n",
    "- **DO NOT** attempt to load large models, such as T5 and GPT-3 on JupyterHub;\n",
    "- **DO NOT** run a program more than 1 week.\n",
    "\n",
    "## 2. PyTorch\n",
    "\n",
    "We use [PyTorch](https://pytorch.org/) framework to finish the implementations. In this section, we will introduce the installation, the basic operations of PyTorch.\n",
    "\n",
    "### 2.1 Installation\n",
    "Here, we install PyTorch with GPU supports. So, we first need to know the CUDA version in the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we find the suitable version package from https://pytorch.org/. Past the command and run in the cell.\n",
    "\n",
    "Here we choose the version 1.10.1 with CUDA 10.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the version of our installed package and whether it supports to GPUs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"GPU support: \", torch.cuda.is_available())\n",
    "print(\"Available devices count: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Quick start - Tensor in PyTorch\n",
    "\n",
    "In this section, we introcue some basic concepts and operations of Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators.\n",
    "\n",
    "### Create Tensors\n",
    "\n",
    "Tensors can be created directly from data or NumPy arrays. You can assign the data type to the tensor. Otherwise, the data type would be automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0,1], [2,3]]\n",
    "tensor_data = torch.tensor(data)\n",
    "tensor_data_float = torch.tensor(data).float()\n",
    "print(f\"Long Tensor: \\n {tensor_data} \\n\")  # the data type is LongTensor\n",
    "print(f\"Float Tensor: \\n {tensor_data_float} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.array(data)\n",
    "tensor_np_data = torch.tensor(np_data)\n",
    "tensor_np_data_float = torch.tensor(np_data).float()\n",
    "print(f\"Long Tensor: \\n {tensor_np_data} \\n\")  # the data type is LongTensor\n",
    "print(f\"Float Tensor: \\n {tensor_np_data_float} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create the tensors filled with constant (e.g., 0 and 1) or random values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_tensor = torch.zeros((2,3))\n",
    "ones_tensor = torch.ones((2,3))\n",
    "random_tensor = torch.rand((2,3))\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Random Tensor: \\n {random_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of a Tensor\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(2,3)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors\n",
    "\n",
    "There are over 100 tensor operations, including arthmetic, linear algebra, matrix manipulation and more. In this section, we only introduce some frequently used operations in our later tutorials and projects.\n",
    "\n",
    "**Move Tensor to Device**\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using `.to()` method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move tensor to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tensor = tensor.to(device)\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor indexing, slicing and reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(4, 6)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let take a look at its first row and column\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:,0]}\")\n",
    "print(f\"Last column: {tensor[:, -1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape\n",
    "print(f\"Reshape to (2,12): \\n {tensor.view(2, 12)} \\n\")\n",
    "print(f\"Reshape to (2,2,6): \\n {tensor.view(-1, 2, 6)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining tensors.** You can use torch.cat to concatenate a sequence of tensors along a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.zeros(4, 2)\n",
    "new_t = torch.cat([tensor, t1, t1], dim=1)\n",
    "new_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmetic operations**\n",
    "\n",
    "The basic arithmetic operations of Pytorch are similar with those in Numpy, such as `.pow()`, `.div()`, `.sum()` and more. Here we talk more about multiplication in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2 will have the same value\n",
    "print(f\"Shape of original tensor: {tensor.shape}\")\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "print(f\"Shape of matrix multiplication resulting tensor: {y1.shape}\")\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "print(f\"Shape of element-wise product resulting tensor: {z1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Practice\n",
    "\n",
    "In NLP, we have a very popular and famous techique, termed **Attention** which is used to measure the improtance among each components. Formally, we define the attention mechanism as:\n",
    "\n",
    "$Attention(\\mathbf{Q},\\mathbf{K},\\mathbf{V}) = \\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}})\\mathbf{V}$\n",
    "\n",
    "$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$,\n",
    "\n",
    "you can attempt to implement softmax function and attention by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v= torch.rand((2,4,8))\n",
    "k = v\n",
    "q = torch.rand((2,4,8))\n",
    "d_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code\n",
    "def softmax(x, dim=0):\n",
    "    pass\n",
    "\n",
    "def attention(q, k, v):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think more** how wo implement self-attention if you only have access to a hidden state **H**. (Mapping **H** with a learnable matrix **W** to **Q, K, V**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
