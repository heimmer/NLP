{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdwDhes8d7X_"
   },
   "source": [
    "# CS6493 - Tutorial 2\n",
    "\n",
    "\n",
    "## Advanced applications with PyTorch - Text classification on AG_NEWS\n",
    "\n",
    "Text classification is a typical NLP task that requires the model to assign a set of predefined categories to open-ended text. Text classifiers can be used to organize, structure, and categorize pretty much any kind of text â€“ from documents, medical studies and files, and all over the web.\n",
    "\n",
    "For example, we use AG_NEWS dataset, which classifies news data into 4 categories: \n",
    "\n",
    "ag_news_label = {0: \"World\",\n",
    "                 1: \"Sports\",\n",
    "                 2: \"Business\",\n",
    "                 3: \"Sci/Tec\"}\n",
    "\n",
    "In this tutorial, we will show how to use the **datasets** to load the raw data and use the **torchtext** to build the dataset for the text classification analysis. You will have the flexibility to\n",
    "\n",
    "   - Access to the raw data\n",
    "   - Build data processing pipeline to convert the raw text strings into ``torch.Tensor`` that can be used to train the model\n",
    "   - Shuffle and iterate the data with `torch.utils.data.DataLoader`\n",
    "---\n",
    "\n",
    "To use torchtext on JupyterHub, you should install the suitable torchtext library regarding to the torch version. Please find the suitable package on https://github.com/pytorch/text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQkpsJOBsQ0-",
    "outputId": "5cecedea-9224-4db5-d848-91410fcf5283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                          1.10.0\n",
      "torchfile                      0.1.0\n",
      "torchlight                     0.0.1\n",
      "torchtext                      0.11.0\n",
      "torchvision                    0.7.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext==0.11.0 in ./.local/lib/python3.6/site-packages (0.11.0)\n",
      "Requirement already satisfied: torch==1.10.0 in ./.local/lib/python3.6/site-packages (from torchtext==0.11.0) (1.10.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.6/site-packages (from torchtext==0.11.0) (4.64.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.11.0) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.11.0) (2.24.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.10.0->torchtext==0.11.0) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.10.0->torchtext==0.11.0) (0.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.11.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.6/site-packages (from requests->torchtext==0.11.0) (1.25.11)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm->torchtext==0.11.0) (3.0.0)\n",
      "Requirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.6/dist-packages (from importlib-resources->tqdm->torchtext==0.11.0) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.6/site-packages (2.4.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.6/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.6/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.6/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./.local/lib/python3.6/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: responses<0.19 in ./.local/lib/python3.6/site-packages (from datasets) (0.17.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.24.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.local/lib/python3.6/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from datasets) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: dill<0.3.6 in ./.local/lib/python3.6/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from responses<0.19->datasets) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm>=4.62.1->datasets) (3.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->datasets) (1.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->datasets) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2020.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDq3SFgod7YB"
   },
   "source": [
    "## Loading raw dataset\n",
    "\n",
    "The Huggingface library provides a large amount of raw datasets. For example, the ``ag_news`` dataset can be download and load with scripts below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ri-hiq8ud7YB",
    "outputId": "f498722e-d7c6-461c-8489-366d88afe4c4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/grads/haochetan2/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8206c8e4c2d48329936060f9f05a9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "from datasets import load_dataset\n",
    "ag_news = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look inside this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ag_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itâ€™s stored inside the `DatasetDict` class and is already split into train and test sets. In order to access each of the splits, we have to just call it with standard sytax of the Python `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 120000\n",
      "})\n",
      "Features of the train_split:  {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=4, names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)} \n",
      "\n",
      "Text in Sample 0:  Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again. \n",
      "\n",
      "Label of Sample 0:  2 \n",
      "\n",
      "Map the Label Back to the Original String: Business \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_split = ag_news['train']\n",
    "print(train_split)\n",
    "print(\"Features of the train_split: \", train_split.features, '\\n')\n",
    "print('Text in Sample 0: ', train_split[0]['text'], '\\n')\n",
    "print('Label of Sample 0: ',train_split[0]['label'], '\\n')\n",
    "print('Map the Label Back to the Original String:', train_split.features['label'].int2str(train_split[0]['label']), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different approaches to access to the `text` in the `Dataset` object. \n",
    "First we can treat it as a **list** that contains all the `text` samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the train_split[\"text\"] is:  <class 'list'>\n",
      "train_split['text'][0]: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
     ]
    }
   ],
   "source": [
    "print('The type of the train_split[\"text\"] is: ', type(train_split['text']))\n",
    "print(\"train_split['text'][0]:\", train_split['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the more powerful applications of ðŸ¤— Datasets come from using the map() function. \n",
    "The primary purpose of map() is to speed up processing functions. \n",
    "It allows you to apply a processing function to each example in a dataset, independently or in batches.\n",
    "\n",
    "Start by creating a function that adds 'News: ' to the beginning of each sentence. The function needs to accept and output a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/grads/haochetan2/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-0cc9297dfb4b9d82.arrow\n",
      "Loading cached processed dataset at /home/grads/haochetan2/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-137331b5570b1f62.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"News: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'News: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
       " \"News: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\",\n",
       " 'News: Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.',\n",
       " 'News: Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_prefix(example):\n",
    "    example[\"text\"] = 'News: ' + example[\"text\"]\n",
    "    return example\n",
    "\n",
    "# Now use map() to apply the add_prefix function to the entire dataset:\n",
    "\n",
    "updated_ag_news = ag_news.map(add_prefix)\n",
    "updated_ag_news['train'][\"text\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy9Qov7Ud7YC"
   },
   "source": [
    "## Prepare data processing pipelines\n",
    "\n",
    "This tutorial uses torchtext to generate dataset, there are three very basic components of the torchtext library, including **tokenizer**, **vocabulary**, **word vectors**. \n",
    "\n",
    "*   A **tokenizer** breaks a stream of text into tokens, usually by looking for whitespace (tabs, spaces, new lines). Here, we use the predefined tokenizer in torchtext. \n",
    "*   The **vocab** object is built based on the train dataset and is used to numericalize tokens into tensors. We represent rare tokens as `<unk>`.\n",
    "*   After mapping each token into a numerical index according to the constructed vocabulary, we then convert the index into **word vectors** in the following model defination part. \n",
    "\n",
    "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function `build_vocab_from_iterator` which accepts iterator that yield list or iterator of tokens. We can also pass any special symbols to be added to the\n",
    "vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-alZ8R4Cd7YD"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for text in data['text']:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_split), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary block converts a list of tokens into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 31, 5298, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6E9H_XMd7YD"
   },
   "source": [
    "Then, we prepare the text processing pipeline with the tokenizer and vocabulary. The text pipeline will be used to process the raw data strings from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yhYfGBmsd7YE"
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NTYVN9-d7YE"
   },
   "source": [
    "The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: 'here is an example.' \n",
      "Word ids: [476, 22, 31, 5298, 2]\n"
     ]
    }
   ],
   "source": [
    "text = \"here is an example.\"\n",
    "print(f\"Raw text: '{text}' \\nWord ids: {text_pipeline(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYnFzZH1d7YF"
   },
   "source": [
    "## Generate data batch and iterator \n",
    "\n",
    "[torch.utils.data.DataLoader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) is recommended for PyTorch users.\n",
    "It works with a map-style dataset that implements the ``getitem()`` and ``len()`` protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of ``False``.\n",
    "\n",
    "Before sending to the model, ``collate_fn`` function works on a batch of samples generated from ``DataLoader``. The input to ``collate_fn`` is a batch of data with the batch size in ``DataLoader``, and ``collate_fn`` processes them according to the data processing pipelines declared previously. Pay attention here and make sure that ``collate_fn`` is declared as a top level definition. This ensures that the function is available in each worker.\n",
    "\n",
    "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of ``nn.EmbeddingBag``. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f927D9Med7YF"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for sample in batch:\n",
    "        _label = sample['label']\n",
    "        _text = sample['text']   \n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
    "\n",
    "train_split = ag_news['train']\n",
    "dataloader = DataLoader(train_split, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAF8S8RRd7YF"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "The model is composed of the [nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer plus a linear layer for the classification purpose. ``nn.EmbeddingBag`` with the default mode of \"mean\" computes the mean value of a â€œbagâ€ of embeddings. Although the text entries here have different lengths, `nn.EmbeddingBag` module requires no padding here since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n",
    "the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n",
    "performance and memory efficiency to process a sequence of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "O5nvP8qdd7YG"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zohDv3yxd7YG"
   },
   "source": [
    "## Initiate an instance\n",
    "\n",
    "The ``AG_NEWS`` dataset has four labels and therefore we essentially work on a four-classes classification task with the labels as following,\n",
    "\n",
    "`{0 : \"World\", 1 : \"Sports\", 2 : \"Business\", 3 : \"Sci/Tec\"}`\n",
    "\n",
    "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cpqgYkTad7YG"
   },
   "outputs": [],
   "source": [
    "num_class = len(set([label for label in train_split['label']]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBoaI1wod7YH"
   },
   "source": [
    "Define functions to train the model and evaluate results.\n",
    "---------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ELuMWxeNd7YH"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeDcFv-yd7YH"
   },
   "source": [
    "Split the dataset and run the model\n",
    "-----------------------------------\n",
    "\n",
    "Since the original AG_NEWS has no *valid* dataset, we split the training\n",
    "dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
    "0.05 (valid). Here we use\n",
    "[torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)\n",
    "function in PyTorch core library.\n",
    "\n",
    "We use the [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>)which combines ``nn.LogSoftmax()`` and ``nn.NLLLoss()`` in a single class, to supervise the training process.\n",
    "It is useful when training a classification problem with C classes.\n",
    "And we use [SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html) with a step learning scheduler [StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR) to update the model parameters. The initial learning rate is set to 5.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjvs569sd7YH",
    "outputId": "8d4b87de-256d-40da-8657-a9e9718507bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1688 batches | accuracy    0.689\n",
      "| epoch   1 |  1000/ 1688 batches | accuracy    0.852\n",
      "| epoch   1 |  1500/ 1688 batches | accuracy    0.877\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 26.48s | valid accuracy    0.880 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1688 batches | accuracy    0.897\n",
      "| epoch   2 |  1000/ 1688 batches | accuracy    0.898\n",
      "| epoch   2 |  1500/ 1688 batches | accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 27.06s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1688 batches | accuracy    0.915\n",
      "| epoch   3 |  1000/ 1688 batches | accuracy    0.916\n",
      "| epoch   3 |  1500/ 1688 batches | accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 26.74s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1688 batches | accuracy    0.923\n",
      "| epoch   4 |  1000/ 1688 batches | accuracy    0.924\n",
      "| epoch   4 |  1500/ 1688 batches | accuracy    0.920\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 26.93s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1688 batches | accuracy    0.931\n",
      "| epoch   5 |  1000/ 1688 batches | accuracy    0.930\n",
      "| epoch   5 |  1500/ 1688 batches | accuracy    0.926\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 26.69s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1688 batches | accuracy    0.941\n",
      "| epoch   6 |  1000/ 1688 batches | accuracy    0.941\n",
      "| epoch   6 |  1500/ 1688 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 26.37s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1688 batches | accuracy    0.945\n",
      "| epoch   7 |  1000/ 1688 batches | accuracy    0.942\n",
      "| epoch   7 |  1500/ 1688 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 26.22s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1688 batches | accuracy    0.945\n",
      "| epoch   8 |  1000/ 1688 batches | accuracy    0.945\n",
      "| epoch   8 |  1500/ 1688 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 26.71s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1688 batches | accuracy    0.945\n",
      "| epoch   9 |  1000/ 1688 batches | accuracy    0.947\n",
      "| epoch   9 |  1500/ 1688 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 26.53s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1688 batches | accuracy    0.945\n",
      "| epoch  10 |  1000/ 1688 batches | accuracy    0.946\n",
      "| epoch  10 |  1500/ 1688 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 26.56s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "  \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_dataset, test_dataset = ag_news['train'], ag_news['test']\n",
    "#train_dataset = to_map_style_dataset(train_iter)\n",
    "#test_dataset = to_map_style_dataset(test_iter)\n",
    "splited_ = train_dataset.train_test_split(test_size = 0.1)\n",
    "split_train_, split_valid_ = splited_['train'], splited_['test']\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwZ02n-od7YI"
   },
   "source": [
    "Evaluate the model with test dataset\n",
    "------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CbNcUlfd7YI"
   },
   "source": [
    "Checking the results of the test datasetâ€¦\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbaeHc_qd7YI",
    "outputId": "9054a09c-d5b7-4af5-bbde-64f3091f7517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.909\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYf6_betd7YI"
   },
   "source": [
    "Test on a random news\n",
    "---------------------\n",
    "\n",
    "Use the best model so far and test a golf news.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGV-Do4ed7YJ",
    "outputId": "75d645dd-47c4-4419-b937-3b16191a88e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {0: \"World\",\n",
    "                 1: \"Sports\",\n",
    "                 2: \"Business\",\n",
    "                 3: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. â€“ Four days ago, Jon Rahm was \\\n",
    "    enduring the seasonâ€™s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursdayâ€™s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering heâ€™d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI4Yzgtg8_Ce"
   },
   "source": [
    "## Practice\n",
    "\n",
    "Please try more experimental settings and hype-parameters to obtain better performance. You can consider from following aspects:\n",
    "\n",
    "- Hype-parameters: batch size, learning rate, training epochs;\n",
    "- The type of optimizer and learning rate scheduler.\n",
    "- More advanced network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial2_Text Classification with TORCHTEXT Library.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
