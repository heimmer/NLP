{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcpAYSP-eNlV"
      },
      "source": [
        "# CS6493 - Tutorial 3\n",
        "\n",
        "## RNNs for Sentiment Analysis on the IMDB dataset\n",
        "\n",
        "In this tutorial, we will introduce how to use RNN networks to achieve sentiment analysis. Sentiment analysis is a well-stuided topic in NLP community. In general, we divide the sentiment analysis into following sub-topics:\n",
        "\n",
        "- **Sentiment analysis with binary classification**: detect if a sentence is positive or negtive;\n",
        "- **Multi-class sentiment analysis**: detect the sentiment of a sentence, such as `happy`, `angery`, `excited`, etc.;\n",
        "- **Aspect-based sentiment analysis**: a fine-grained sentiment task which focuses on the sentiment polarity of a specific aspect;\n",
        "- **Multi-modal sentiment analysis**: detect sentiments based on the text, audio and images.\n",
        "\n",
        "Here, we give a simple demonstration of binary classification sentiment analysis on movie reviews, using the [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "**Reminder:** Please check your experimental environment at frist. We will use `torch==1.10.0` and `datasets` in this demo.\n",
        "\n",
        "We will use following technologies in this tutorial,\n",
        "\n",
        "- pre-trained word embeddings, i.e., `Glove`;\n",
        "- packed padded sequence;\n",
        "- `bidirectional LSTM`;\n",
        "- regularization, i.e., `Dropout`;\n",
        "- the optimizer `Adam`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uykIV3zEeNlY",
        "outputId": "f513a91e-fd07-4d36-f2c1-d8d32c758eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1xM6EcBeNla"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import datasets\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WbYUblxeNla",
        "outputId": "2e39d232-e37a-41aa-ece3-16270abf12fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa4bfa82930>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzJz2ep7eNlb"
      },
      "source": [
        "## Preparing data\n",
        "\n",
        "Firstly, we use the `.load_dataset()` to load the IMDB dataset. Note that since the released dataset does not contain the validation dataset, we need to split the training set for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b5b795da4e5d4ad194a6bd18f8282f1f",
            "04e4ded8a8b8404ba53b05367032b73f",
            "1333282fe3dd4e148b84d2e5d0d9cf2c",
            "5aec2cb13b3c44bbbd4fc6b91801c20d",
            "07fe29f8b96842b28298bf4cf9035a14",
            "1648db01b4ad4aa0b1587319b0874798",
            "0f67bc2901ab4562a84634d5b25f6a44",
            "1a882fa5590a4c8cb40d8e6ee255bcb2",
            "ece4dbdbd0ee4998ba66cbe787a9e919",
            "082b14f39ce14dfead4bacd87168d0ab",
            "4c160679e267406e89b33068d89cc45e"
          ]
        },
        "id": "ksiOW74WeNlb",
        "outputId": "15809f5e-22a2-42a4-f751-82b5f41ec474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5b795da4e5d4ad194a6bd18f8282f1f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_data, test_data = datasets.load_dataset('imdb', split=['train', 'test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHOOO2FzeNlb"
      },
      "source": [
        "We can also manually download it and then upload it to our server, and load it via `datasets.load_from_disk()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INX8QrlaeNlc"
      },
      "outputs": [],
      "source": [
        "# train_data = datasets.load_from_disk('imdb')['train']\n",
        "# test_data = datasets.load_from_disk('imdb')['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7-XnLLQeNlc"
      },
      "source": [
        "We crop the long sequence with `max_length=256` and filter the rare words with the `min_freq=5`. We will use `pack_padded_sequence()`, which make our RNNs only process the non-padded elements of our sequence. To this end, we have to tell the RNNs how long the actual sequences are. We do this by recording the length of each sequence during the pre-processing stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88dCVWSteNld",
        "outputId": "fe899096-dd20-4970-84d8-d5fa4e48f536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-efeab427d66d4c2d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-68bf9a4baf3d6088.arrow\n"
          ]
        }
      ],
      "source": [
        "max_length = 256\n",
        "min_freq = 5\n",
        "special_tokens = ['<unk>', '<pad>']\n",
        "\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "def tokenize_data(example):\n",
        "    tokens = tokenizer(example['text'])[:max_length]\n",
        "    length = len(tokens)\n",
        "    example['tokens'] = tokens\n",
        "    example['length'] = length\n",
        "    return example\n",
        "\n",
        "train_data = train_data.map(tokenize_data)\n",
        "test_data = test_data.map(tokenize_data)\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for item in data_iter:\n",
        "        yield item['tokens']\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_data), min_freq=min_freq, specials=special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNcaq9qreNld"
      },
      "outputs": [],
      "source": [
        "unk_index = vocab['<unk>'] # unknown token\n",
        "pad_index = vocab['<pad>'] # pad token\n",
        "vocab.set_default_index(unk_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngBcYDQoeNle"
      },
      "source": [
        "Splitting the original training data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm-hl0QjeNle",
        "outputId": "ed3f0181-d914-4869-f06d-50b0b4499e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-5036cffc9836bba2.arrow and /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-d467d5c317d7310f.arrow\n"
          ]
        }
      ],
      "source": [
        "splited_ = train_data.train_test_split(test_size = 0.1)\n",
        "train_data, valid_data = splited_['train'], splited_['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SjhHIyNeNle",
        "outputId": "d4377790-e3e8-447b-8da4-915088e8cb73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label', 'tokens', 'length'],\n",
              "    num_rows: 22500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSpa50LSeNle",
        "outputId": "4edfd6ce-1661-425f-b192-e6c5e2bb6f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-0f903c5cabdc7bb9.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-a80195f6eede761d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-9275b17fc7bc0ef9.arrow\n"
          ]
        }
      ],
      "source": [
        "def vectorize_data(example):\n",
        "    ids = [vocab[token] for token in example['tokens']]\n",
        "    example['ids'] = ids\n",
        "    return example\n",
        "\n",
        "train_data = train_data.map(vectorize_data)\n",
        "valid_data = valid_data.map(vectorize_data)\n",
        "test_data = test_data.map(vectorize_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzPtHJ1BeNlf"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "We use the Long Short-Term Memory (LSTM) which use multiple gates to control the flow of information into or out of the memory. There are some key points:\n",
        "\n",
        "- `bidirectional`: Default **FALSE**. Set **True** to use the bi-LSTM;\n",
        "- You are encouraged to give an initialization to LSTM, which would benefit to the convegence speed;\n",
        "- To alleviate the overfitting, we introduce `dropout` into our model. Dropout works by randomly dropping out neurons in a layer during a forward pass;\n",
        "- In batch training, we need to use `pack_padded_sequence()` to help the model only process the non-padded elements of a sequence. When using this function, the `hidden` and `cell` are both from the last non-padded element in the sequence. Otherwise, they would be from the last element in the sequence, which probably would be a pad token.\n",
        "- The `lengths` argument of `packed_padded_sequence` must be a CPU tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BqRpyJPeNlf"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class, n_layers, bidirectional,\n",
        "                 dropout_rate, pad_index):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,\n",
        "                            dropout=dropout_rate, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_class)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.initialize_weights()\n",
        "    \n",
        "    def initialize_weights(self):\n",
        "        nn.init.xavier_normal_(self.embedding.weight)\n",
        "        nn.init.xavier_normal_(self.fc.weight)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if \"bias\" in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif \"weight\" in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "        \n",
        "    def forward(self, ids, length):\n",
        "        # ids = [batch size, seq len]\n",
        "        # length = [batch size]\n",
        "        embedded = self.dropout(self.embedding(ids))\n",
        "        # embedded = [batch size, seq len, embedding dim]\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True, \n",
        "                                                            enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        # output = [batch size, seq len, hidden dim * n directions]\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
        "            # hidden = [batch size, hidden dim * 2]\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1])\n",
        "            # hidden = [batch size, hidden dim]\n",
        "        prediction = self.fc(hidden)\n",
        "        # prediction = [batch size, output dim]\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNe0-RLbeNlf"
      },
      "source": [
        "Now, we need to create a model instance with the specifc arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5W-DJy8eNlf"
      },
      "outputs": [],
      "source": [
        "# model parameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 300\n",
        "num_class = 2 # negtive/positive\n",
        "n_layers = 2\n",
        "bidirectional = True # use bi-lstm\n",
        "dropout_rate = 0.5\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_class, n_layers, bidirectional, dropout_rate, \n",
        "             pad_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2YM1wZLeNlg"
      },
      "source": [
        "Let us take a look at the total parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8bAr4U_eNlg",
        "outputId": "91a9371c-eb73-4822-ee9a-421596f50fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 11,079,902 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhg5o3HLeNlg"
      },
      "source": [
        "Instead of randomly initialize the word embeddings, we here use the pretrained word embeddings `GLOVE.6B.100D`. Alternatively, you can try some other word embeddings following [this page](https://pytorch.org/text/stable/vocab.html#pretrained-word-embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaOJ1v3meNlg"
      },
      "outputs": [],
      "source": [
        "glove_embed = torchtext.vocab.GloVe(name=\"6B\", dim=300) # vocab_size: 6B; word dimension: 300.\n",
        "pretrained_word_embeddings = glove_embed.get_vecs_by_tokens(vocab.get_itos())\n",
        "model.embedding.weight.data = pretrained_word_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtVRA_38eNlg"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We use the `Adam` optimizer to update the model parameters. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. More information about Adam (and other optimizers) can be found [here](http://ruder.io/optimizing-gradient-descent/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmXMz61JeNlh"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    batch_ids = [torch.tensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = [torch.tensor(i['length']) for i in batch]\n",
        "    batch_length = torch.stack(batch_length)\n",
        "    batch_label = [torch.tensor(i['label']) for i in batch]\n",
        "    batch_label = torch.stack(batch_label)\n",
        "    batch = {'ids': batch_ids,\n",
        "             'length': batch_length,\n",
        "             'label': batch_label}\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xevpK4veNlh"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, criterion, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...'):\n",
        "        ids = batch['ids'].to(device)\n",
        "        length = batch['length']\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids, length)\n",
        "        loss = criterion(prediction, label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...'):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29EhDiIpeNlh"
      },
      "source": [
        "During the training stage, we save the model that performs best on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qIOpdqseNlh",
        "outputId": "f4c711a3-c565-427e-f734-5d7ee6401c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:44<00:00,  1.97it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "train_loss: 0.593, train_acc: 0.673\n",
            "valid_loss: 0.426, valid_acc: 0.798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:44<00:00,  1.99it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2\n",
            "train_loss: 0.456, train_acc: 0.789\n",
            "valid_loss: 0.461, valid_acc: 0.785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:45<00:00,  1.93it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 3\n",
            "train_loss: 0.449, train_acc: 0.800\n",
            "valid_loss: 0.355, valid_acc: 0.848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:45<00:00,  1.92it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 4\n",
            "train_loss: 0.351, train_acc: 0.851\n",
            "valid_loss: 0.361, valid_acc: 0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:46<00:00,  1.89it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 5\n",
            "train_loss: 0.415, train_acc: 0.820\n",
            "valid_loss: 0.589, valid_acc: 0.617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:46<00:00,  1.90it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 6\n",
            "train_loss: 0.522, train_acc: 0.722\n",
            "valid_loss: 0.357, valid_acc: 0.857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:46<00:00,  1.90it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 7\n",
            "train_loss: 0.346, train_acc: 0.853\n",
            "valid_loss: 0.331, valid_acc: 0.865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:46<00:00,  1.89it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 8\n",
            "train_loss: 0.315, train_acc: 0.869\n",
            "valid_loss: 0.362, valid_acc: 0.847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:46<00:00,  1.88it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 9\n",
            "train_loss: 0.277, train_acc: 0.887\n",
            "valid_loss: 0.293, valid_acc: 0.882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training...: 100%|██████████| 88/88 [00:46<00:00,  1.88it/s]\n",
            "evaluating...: 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10\n",
            "train_loss: 0.235, train_acc: 0.909\n",
            "valid_loss: 0.290, valid_acc: 0.881\n"
          ]
        }
      ],
      "source": [
        "# training hyper-parameters\n",
        "n_epochs = 10\n",
        "lr = 5e-4\n",
        "batch_size = 256\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# train_data = to_map_style_dataset(train_data)\n",
        "# valid_data = to_map_style_dataset(valid_data)\n",
        "# test_data = to_map_style_dataset(test_data)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, device)\n",
        "    valid_loss, valid_acc = evaluate(valid_loader, model, criterion, device)\n",
        "\n",
        "    train_losses.extend(train_loss)\n",
        "    train_accs.extend(train_acc)\n",
        "    valid_losses.extend(valid_loss)\n",
        "    valid_accs.extend(valid_acc)\n",
        "    \n",
        "    epoch_train_loss = np.mean(train_loss)\n",
        "    epoch_train_acc = np.mean(train_acc)\n",
        "    epoch_valid_loss = np.mean(valid_loss)\n",
        "    epoch_valid_acc = np.mean(valid_acc)\n",
        "    \n",
        "    if epoch_valid_loss < best_valid_loss:\n",
        "        best_valid_loss = epoch_valid_loss\n",
        "        torch.save(model.state_dict(), 'lstm.pt')\n",
        "    \n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "    print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1BL3BDIeNlh"
      },
      "source": [
        "## Evaluate on the test set\n",
        "\n",
        "Now, we load the well-trained model and evaluate on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqleUpmSeNlh",
        "outputId": "96df284d-e6b0-4a8a-9acb-39d20378de66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "evaluating...: 100%|██████████| 98/98 [00:21<00:00,  4.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_loss: 0.331, test_acc: 0.861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('lstm.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(test_loader, model, criterion, device)\n",
        "\n",
        "epoch_test_loss = np.mean(test_loss)\n",
        "epoch_test_acc = np.mean(test_acc)\n",
        "\n",
        "print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPpmJWuveNli"
      },
      "source": [
        "## Test on a random example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsVod0JUeNli"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
        "    tokens = tokenizer(text)\n",
        "    ids = [vocab[t] for t in tokens]\n",
        "    length = torch.LongTensor([len(ids)])\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor, length).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    return predicted_class, predicted_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBzH2j3zeNli",
        "outputId": "a52a345c-977e-462a-c5c9-c3fac724d674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'This film is terrible!' is negtive with probability 0.9612252712249756.\n"
          ]
        }
      ],
      "source": [
        "text = \"This film is terrible!\"\n",
        "\n",
        "predicted_label, prob = predict_sentiment(text, model, tokenizer, vocab, device)\n",
        "print(f\"'{text}' is {'positive' if predicted_label else 'negtive'} with probability {prob}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxrZv0lmeNli"
      },
      "source": [
        "## Practice\n",
        "\n",
        "Please try more experimental settings and hype-parameters to obtain better performance. You can consider from following aspects:\n",
        "\n",
        "- Based on the comparisons of traning and validation accuracy, we can find that the model is still overfitting to the training set. So, try to alleviate the overfitting.\n",
        "- In our example, we use `LSTM` as the backbone. Please try some other RNNs, like `GRU`.\n",
        "- Please try more pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzeyYIW9eNli",
        "outputId": "5a37cd1b-7c27-4620-b3d2-64b3671db180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 30 07:00:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxlJRQzKeNli"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5b795da4e5d4ad194a6bd18f8282f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04e4ded8a8b8404ba53b05367032b73f",
              "IPY_MODEL_1333282fe3dd4e148b84d2e5d0d9cf2c",
              "IPY_MODEL_5aec2cb13b3c44bbbd4fc6b91801c20d"
            ],
            "layout": "IPY_MODEL_07fe29f8b96842b28298bf4cf9035a14"
          }
        },
        "04e4ded8a8b8404ba53b05367032b73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1648db01b4ad4aa0b1587319b0874798",
            "placeholder": "​",
            "style": "IPY_MODEL_0f67bc2901ab4562a84634d5b25f6a44",
            "value": "100%"
          }
        },
        "1333282fe3dd4e148b84d2e5d0d9cf2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a882fa5590a4c8cb40d8e6ee255bcb2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ece4dbdbd0ee4998ba66cbe787a9e919",
            "value": 2
          }
        },
        "5aec2cb13b3c44bbbd4fc6b91801c20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_082b14f39ce14dfead4bacd87168d0ab",
            "placeholder": "​",
            "style": "IPY_MODEL_4c160679e267406e89b33068d89cc45e",
            "value": " 2/2 [00:00&lt;00:00, 60.16it/s]"
          }
        },
        "07fe29f8b96842b28298bf4cf9035a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1648db01b4ad4aa0b1587319b0874798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f67bc2901ab4562a84634d5b25f6a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a882fa5590a4c8cb40d8e6ee255bcb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece4dbdbd0ee4998ba66cbe787a9e919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "082b14f39ce14dfead4bacd87168d0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c160679e267406e89b33068d89cc45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}